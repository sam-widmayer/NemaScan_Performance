---
title: "Polygenic Architectures are Difficult to Resolve Using GWAS"
site: workflowr::wflow_site
output:
  workflowr::wflow_html:
editor_options:
  chunk_output_type: console
---
```{r setup, message=FALSE, warning=FALSE, include=FALSE}
require(tidyverse)
require(tidymodels)
require(nationalparkcolors)
require(RColorBrewer)
require(workflowr)
require(viridis)
require(wesanderson)
require(data.table)
```

### Analysis date: `r format(Sys.time(), '%B %d, %Y')`

```{r message=FALSE, warning=FALSE, include=FALSE}
setwd("~/Documents/projects/NemaScan_Performance/") # Lab Computer
# setwd("~/Documents/AndersenLab/NemaScan_Performance/") # Laptop

# Dual Algorithm Aggregated Mapping Results: Default settings: Group = 1000 bp, CI = +150 markers
load(file = "data/NemaScan_Performance.Architecture.Mixed.20201223.RData") 
#####
dat.group1000.150.aggregate <- simulation.metrics.df %>%
  tidyr::separate(col = sim,
                  into = c("nQTL","Rep","h2","MAF","effect_range","strain_set"), 
                  sep = "_", remove = F) %>%
  tidyr::separate(col = QTL,
                  into = c("CHROM","POS"), 
                  sep = ":", remove = F) %>%
  dplyr::mutate(h2 = as.factor(h2),
                nQTL = as.factor(nQTL),
                POS = as.numeric(POS),
                startPOS = as.numeric(startPOS),
                peakPOS = as.numeric(peakPOS),
                endPOS = as.numeric(endPOS),
                var.exp  = as.numeric(var.exp), # watch
                Simulated.QTL.VarExp = as.numeric(Simulated.QTL.VarExp), # watch
                peak_id = as.numeric(peak_id),
                BETA = as.numeric(BETA),
                Effect = as.numeric(Effect),
                Frequency = as.numeric(Frequency),
                log10p = as.numeric(log10p),
                interval_size = as.numeric(interval_size)) %>%
  dplyr::filter(!algorithm %in% c("No Successful Mappings"),
                CHROM != 7)
dat.group1000.150.aggregate$nQTL <- factor(dat.group1000.150.aggregate$nQTL, levels = c("1","5","10","25"))
dat.group1000.150.aggregate$group.size <- as.factor(1000)
dat.group1000.150.aggregate$CI <- as.factor(150)
dat.group1000.150.aggregate$eval.method <- "Joint"
#####

# Group = 500 bp, CI = +150 markers
load(file = "data/NemaScan_Performance.Architecture.Mixed.group500.20201223.RData") #
#####
dat.group500.150.aggregate <- simulation.metrics.df %>%
  tidyr::separate(col = sim,
                  into = c("nQTL","Rep","h2","MAF","effect_range","strain_set"), 
                  sep = "_", remove = F) %>%
  tidyr::separate(col = QTL,
                  into = c("CHROM","POS"), 
                  sep = ":", remove = F) %>%
  dplyr::mutate(h2 = as.factor(h2),
                nQTL = as.factor(nQTL),
                POS = as.numeric(POS),
                startPOS = as.numeric(startPOS),
                peakPOS = as.numeric(peakPOS),
                endPOS = as.numeric(endPOS),
                var.exp  = as.numeric(var.exp), # watch
                Simulated.QTL.VarExp = as.numeric(Simulated.QTL.VarExp), # watch
                peak_id = as.numeric(peak_id),
                BETA = as.numeric(BETA),
                Effect = as.numeric(Effect),
                Frequency = as.numeric(Frequency),
                log10p = as.numeric(log10p),
                interval_size = as.numeric(interval_size)) %>%
  dplyr::filter(!algorithm %in% c("No Successful Mappings"),
                CHROM != 7)
dat.group500.150.aggregate$nQTL <- factor(dat.group500.150.aggregate$nQTL, levels = c("1","5","10","25"))
dat.group500.150.aggregate$group.size <- as.factor(500)
dat.group500.150.aggregate$CI <- as.factor(150)
#####

# Group = 1000 bp, CI = +50 markers
load(file = "data/NemaScan_Performance.Architecture.Mixed.CI.50.20201224.RData")
#####

dat.group1000.50.aggregate <- simulation.metrics.df %>%
  tidyr::separate(col = sim,
                  into = c("nQTL","Rep","h2","MAF","effect_range","strain_set"),
                  sep = "_", remove = F) %>%
  tidyr::separate(col = QTL,
                  into = c("CHROM","POS"),
                  sep = ":", remove = F) %>%
  dplyr::mutate(h2 = as.factor(h2),
                nQTL = as.factor(nQTL),
                POS = as.numeric(POS),
                startPOS = as.numeric(startPOS),
                peakPOS = as.numeric(peakPOS),
                endPOS = as.numeric(endPOS),
                var.exp  = as.numeric(var.exp), # watch
                Simulated.QTL.VarExp = as.numeric(Simulated.QTL.VarExp), # watch
                peak_id = as.numeric(peak_id),
                BETA = as.numeric(BETA),
                Effect = as.numeric(Effect),
                Frequency = as.numeric(Frequency),
                log10p = as.numeric(log10p),
                interval_size = as.numeric(interval_size)) %>%
  dplyr::filter(!algorithm %in% c("No Successful Mappings"),
                CHROM != 7)
dat.group1000.50.aggregate$nQTL <- factor(dat.group1000.50.aggregate$nQTL, levels = c("1","5","10","25"))
dat.group1000.50.aggregate$group.size <- as.factor(1000)
dat.group1000.50.aggregate$CI <- as.factor(50)
#####

# Group = 500 bp, CI = +50 markers
load(file = "data/NemaScan_Performance.Architecture.Mixed.group500.CI.50.20201224.RData")
#####
 # Architecture Sims
dat.group500.50.aggregate <- simulation.metrics.df %>%
  tidyr::separate(col = sim,
                  into = c("nQTL","Rep","h2","MAF","effect_range","strain_set"),
                  sep = "_", remove = F) %>%
  tidyr::separate(col = QTL,
                  into = c("CHROM","POS"),
                  sep = ":", remove = F) %>%
  dplyr::mutate(h2 = as.factor(h2),
                nQTL = as.factor(nQTL),
                POS = as.numeric(POS),
                startPOS = as.numeric(startPOS),
                peakPOS = as.numeric(peakPOS),
                endPOS = as.numeric(endPOS),
                var.exp  = as.numeric(var.exp), # watch
                Simulated.QTL.VarExp = as.numeric(Simulated.QTL.VarExp), # watch
                peak_id = as.numeric(peak_id),
                BETA = as.numeric(BETA),
                Effect = as.numeric(Effect),
                Frequency = as.numeric(Frequency),
                log10p = as.numeric(log10p),
                interval_size = as.numeric(interval_size)) %>%
  dplyr::filter(!algorithm %in% c("No Successful Mappings"),
                CHROM != 7)
dat.group500.50.aggregate$nQTL <- factor(dat.group500.50.aggregate$nQTL, levels = c("1","5","10","25"))
dat.group500.50.aggregate$group.size <- as.factor(500)
dat.group500.50.aggregate$CI <- as.factor(50)
#####

# Combining iterations of grouping parameter and CI extension length
dat <- dat.group1000.150.aggregate %>%
  dplyr::full_join(., dat.group500.150.aggregate) %>%
  dplyr::full_join(., dat.group1000.50.aggregate) %>%
  dplyr::full_join(., dat.group500.50.aggregate)
dat$group.size <- factor(dat$group.size, levels = c(500,1000))
dat$CI <- factor(dat$CI, levels = c(50,150))
```

### NemaScan simulation performance was assessed with the following experimental parameters:
* ###### Number of Simulated QTL: ```r levels(as.factor(dat$nQTL))```
* ###### Sample Population(s): ```r levels(as.factor(dat$strain_set))```
* ###### Heritability(ies): ```r levels(as.factor(dat$h2))```
* ###### MAF(s): ```r levels(as.factor(dat$MAF))```
* ###### Number of Replicates per Regime: ```r max(as.numeric(dat$Rep))```
* ###### QTL Effect Range: ```r levels(as.factor(dat$effect_range))```

```{r simulated QTL locations, echo=FALSE}
h2.pal <- wes_palette("Darjeeling1", 4)[c(1,4,3,2)]
nQTL.pal <- wes_palette("Darjeeling2", 4)[c(3,1,4,2)]
options(dplyr.summarise.inform = FALSE)
dat.group1000.150.aggregate %>%
  dplyr::filter(Simulated == TRUE,
                !duplicated(QTL)) %>%
  ggplot(mapping = aes(x = POS/1000000, y = Effect, fill = nQTL)) + 
  theme_bw() + 
  geom_point(shape = 21, alpha = 0.75) + 
  facet_grid(eval.method~CHROM, drop = TRUE, scales = "free") + 
  scale_fill_manual(values = nQTL.pal) + 
  labs(x = "Genomic position (Mb)",
       y = "Causal QTL Effect",
       title = "Where were QTL Simulated?")

dat.group1000.150.aggregate %>%
  dplyr::filter(Simulated == TRUE,
                !duplicated(QTL)) %>%
  ggplot(., mapping = aes(x = Frequency, fill = nQTL)) +
  theme_bw() + 
  geom_density() + 
  theme(legend.position = "top") + 
      facet_grid(eval.method~nQTL, drop = TRUE, scales = "free") + 
  scale_fill_manual(values = nQTL.pal) + 
  xlim(c(0.05,0.5)) + 
  labs(x = "Minor Allele Frequency",
       y = "Frequency of Simulated QTL (Smoothed Density)")

dat.group1000.150.aggregate %>%
  dplyr::filter(Simulated == TRUE) %>%
  ggplot(., mapping = aes(x = Simulated.QTL.VarExp, fill = nQTL)) +
  theme_bw() + 
  geom_density() + 
  theme(legend.position = "top") + 
      facet_grid(nQTL~eval.method+h2, drop = TRUE) + 
  scale_fill_manual(values = nQTL.pal) + 
  labs(x = "Variance Explained by Simulated QTL",
       y = "Frequency of Simulated QTL (Smoothed Density)")
```

```{r default settings PR curve, echo=FALSE, fig.height=6, fig.width=9}
pr.curve <- dat.group1000.150.aggregate %>%
  dplyr::group_by(h2, nQTL, algorithm) %>%
  yardstick::pr_curve(Simulated, log10p) %>%
  dplyr::filter(!is.infinite(.threshold))

ggplot(pr.curve, mapping = aes(x = recall, y = precision, colour = nQTL)) + 
  theme_bw() + 
  geom_path() +
  theme(strip.text = element_text(size = 8),
        axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1),
        panel.grid.minor = element_blank()) +
  theme(legend.position = "top") + 
  scale_colour_brewer(palette = "OrRd", name = "Number of Supporting QTL") +
  facet_grid(h2~algorithm) + 
  labs(x = "Recall", y = "Precision")
```

```{r default settings metrics, echo=FALSE}
options(dplyr.summarise.inform = FALSE)
power <- dat.group1000.150.aggregate %>%
  dplyr::group_by(h2, algorithm, nQTL, group.size, CI) %>%
  yardstick::sens(truth = Simulated, estimate = Detected) %>%
  dplyr::mutate(.metric = "Power")
precision <- dat.group1000.150.aggregate %>%
  dplyr::group_by(h2, algorithm, nQTL, group.size, CI) %>%
  yardstick::precision(truth = Simulated, estimate = Detected) %>%
  dplyr::mutate(.metric = "Precision")
pr_auc <- dat.group1000.150.aggregate %>%
  dplyr::group_by(h2, algorithm, nQTL, group.size, CI) %>%
  yardstick::pr_auc(Simulated, log10p) %>%
  dplyr::mutate(.metric = "Precision-Recall_AUC")
resolution <- dat.group1000.150.aggregate %>%
  dplyr::filter(Simulated == TRUE) %>%
  dplyr::filter(!is.na(var.exp)) %>%
  dplyr::select(CHROM, POS, Simulated, Frequency, interval_size, algorithm, h2, Simulated.QTL.VarExp, var.exp, nQTL, group.size, CI, algorithm) %>%
  dplyr::group_by(h2, algorithm, nQTL, group.size, CI) %>%
  dplyr::summarise(mean(interval_size), median(interval_size), sd(interval_size), 
                   mean(Simulated.QTL.VarExp), median(Simulated.QTL.VarExp), sd(Simulated.QTL.VarExp))

summarized <- power %>%
  dplyr::full_join(., precision) %>%
  dplyr::full_join(., pr_auc) %>%
  tidyr::pivot_wider(names_from = .metric, values_from = .estimate) %>%
  dplyr::mutate(F1score = 2*((Precision*Power)/(Precision+Power)),
                FDR = 1-Precision) %>%
  dplyr::left_join(.,resolution) %>%
  dplyr::mutate(mean.interval.Mb = `mean(interval_size)`/1000000,
                median.interval.Mb = `median(interval_size)`/1000000,
                sd.interval.Mb = `sd(interval_size)`/1000000, 
                mean.PVE = `mean(Simulated.QTL.VarExp)`*100,
                median.PVE = `median(Simulated.QTL.VarExp)`*100,
                sd.PVE = `sd(Simulated.QTL.VarExp)`*100, 
                nQTL = as.factor(nQTL),
                .keep = "unused")
```

```{r plots, echo=FALSE}
## PLOTS
sd.POWER <- dat.group1000.150.aggregate %>%
  dplyr::filter(algorithm == "MIXED") %>%
  droplevels() %>%
  dplyr::group_by(h2, algorithm, nQTL, group.size, CI, Rep) %>%
  yardstick::sens(truth = Simulated, estimate = Detected) %>%
  dplyr::mutate(.metric = "Power") %>%
  dplyr::group_by(algorithm, h2, nQTL, group.size, CI) %>%
  dplyr::summarise(mean = mean(.estimate),
                   sd = sd(.estimate)) %>%
  dplyr::mutate(b = mean + sd,
                a = mean - sd,
                nQTL = as.factor(nQTL),
                ymax = if_else(condition = b > 1, 
                             true = 1-mean, 
                             false = sd),
                ymin = if_else(condition = a < 0, 
                             true = (0-mean)*-1, 
                             false = sd)) %>%
  dplyr::select(-a,-b)

sd.FDR <- dat.group1000.150.aggregate %>%
  dplyr::filter(algorithm == "MIXED") %>%
  droplevels() %>%
  dplyr::group_by(h2, algorithm, nQTL, group.size, CI, Rep) %>%
  yardstick::precision(truth = Simulated, estimate = Detected) %>%
  dplyr::select(-.metric, -.estimator) %>%
  dplyr::mutate(FDR = 1-.estimate) %>%
  dplyr::select(-.estimate) %>%
  dplyr::filter(!is.na(FDR)) %>%
  dplyr::group_by(algorithm, h2, nQTL, group.size, CI) %>%
  dplyr::summarise(mean = mean(FDR),
                   sd = sd(FDR)) %>%
  dplyr::mutate(b = mean + sd,
                a = mean - sd,
                nQTL = as.factor(nQTL),
                ymax = if_else(condition = b > 1,
                             true = 1-mean,
                             false = sd),
                ymin = if_else(condition = a < 0,
                             true = (0-mean)*-1,
                             false = sd)) %>%
  dplyr::select(-a,-b)


power.plot <- sd.POWER %>%
  ggplot(., mapping = aes(x = nQTL, y = mean, colour = h2, 
                          group = interaction(h2,algorithm))) +
  theme_bw() +
  geom_point(position = position_dodge(width = 0.2)) +
  geom_errorbar(data = sd.POWER,
                mapping = aes(y = mean, ymax = mean+ymax, ymin = mean-ymin),
                width = 0.2,
                position=position_dodge(width=0.2)) + 
  geom_line() +
  scale_colour_manual(values = h2.pal, name = expression(italic(h^2))) +
  scale_alpha_manual(values = c(1,0.5,0.5), name = "Evaluation Method") +
  ylim(c(0,1)) + 
  theme(strip.text = element_text(size = 8),
        legend.position = "top",
        panel.grid.minor = element_blank()) +
  labs(x = "Number of Supporting QTL",
       y = "Power")
power.plot


FDR.plot <- sd.FDR %>%
  ggplot(., mapping = aes(x = nQTL, y = mean, colour = h2, group = interaction(h2,algorithm))) +
  theme_bw() +
  geom_point(position=position_dodge(width=0.2)) +
  geom_errorbar(data = sd.FDR,
                mapping = aes(y = mean, ymax = mean+ymax, ymin = mean-ymin),
                width = 0.2,
                position=position_dodge(width=0.2)) + 
  # geom_line(aes(linetype = algorithm)) +
  geom_line(position=position_dodge(width=0.2)) +
  scale_colour_manual(values = h2.pal, name = expression(italic(h^2))) +
  scale_alpha_manual(values = c(1,0.5,0.5), name = "Evaluation Method") +
  # scale_linetype_manual(values = c(3,2,1), name = "Evaluation Method") +
  # facet_grid(group.size ~ CI) + 
  ylim(c(0,1)) +
  theme(strip.text = element_text(size = 8),
        legend.position = "right",
        panel.grid.minor = element_blank()) +
  labs(x = "Number of Supporting QTL",
       y = "FDR")
FDR.plot

plots <- cowplot::plot_grid(power.plot+ theme(legend.position = "none"),
                            FDR.plot+ theme(legend.position = "none"),
                            nrow = 1,
                            labels = "AUTO", align = "hv", axis = "b")

h2.legend <- cowplot::get_legend(power.plot)
legends <- cowplot::plot_grid(h2.legend, ncol = 1)
prelim.fig.2 <- cowplot::plot_grid(plots, 
                                   legends, 
                                   ncol = 1, 
                                   rel_heights = c(6,1))
ggsave(plot = prelim.fig.2, filename = "output/prelim.fig.2.png", height = 4, width = 10)
prelim.fig.2
```



## TABLES
```{r eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
options(dplyr.summarise.inform = FALSE)
# Which algorithm has greater power
print("POWER EVALUATION") # NOTE: WINNER HERE IS HIGHER
power.comp.algorithm <- dat %>%
  dplyr::group_by(h2, algorithm, nQTL, group.size, CI) %>%
  yardstick::sens(truth = Simulated, estimate = Detected) %>%
  dplyr::mutate(.metric = "Power") %>%
  tidyr::pivot_wider(names_from = algorithm, values_from = .estimate) %>%
  dplyr::select(-.metric, -.estimator) %>%
  dplyr::mutate(winner = if_else(condition = `LMM-EXACT-LOCO` <= `LMM-EXACT-INBRED`,
                                 true = "INBRED",
                                 false = "LOCO")) %>%
  dplyr::group_by(group.size, CI) %>%
  tidyr::nest()

for(i in 1:length(power.comp.algorithm$data)){
  print(paste0("Group Size: ", as.character(power.comp.algorithm$group.size[[i]])))
  print(paste0("CI Extension: ", as.character(power.comp.algorithm$CI[[i]])))
  print(power.comp.algorithm$data[[i]])
}


power.comp.group.size <- dat.group1000.150.aggregate %>%
  dplyr::group_by(h2, algorithm, nQTL, group.size, CI) %>%
  yardstick::sens(truth = Simulated, estimate = Detected) %>%
  dplyr::mutate(.metric = "Power") %>%
  tidyr::pivot_wider(names_from = group.size, values_from = .estimate) %>%
  dplyr::select(-.metric, -.estimator) %>%
  dplyr::mutate(winner = if_else(condition = `500` <= `1000`,
                                 true = "1000",
                                 false = "500")) %>%
  dplyr::arrange(nQTL) %>%
  dplyr::group_by(algorithm, CI) %>%
  tidyr::nest()

for(i in 1:length(power.comp.group.size$data)){
  print(paste0("CI Extension: ", as.character(power.comp.group.size$CI[[i]])))
  print(paste0("Algorithm: ", as.character(power.comp.group.size$algorithm[[i]])))
  print(power.comp.group.size$data[[i]])
}

power.comp.CI <- dat %>%
  dplyr::group_by(h2, algorithm, nQTL, group.size, CI) %>%
  yardstick::sens(truth = Simulated, estimate = Detected) %>%
  dplyr::mutate(.metric = "Power") %>%
  tidyr::pivot_wider(names_from = CI, values_from = .estimate) %>%
  dplyr::select(-.metric, -.estimator) %>%
  dplyr::mutate(winner = if_else(condition = `50` <= `150`,
                                 true = "150",
                                 false = "50")) %>%
  dplyr::arrange(nQTL) %>%
  dplyr::group_by(algorithm, group.size) %>%
  tidyr::nest()

for(i in 1:length(power.comp.CI$data)){
  print(paste0("Group Size: ", as.character(power.comp.CI$group.size[[i]])))
  print(paste0("Algorithm: ", as.character(power.comp.CI$algorithm[[i]])))
  print(power.comp.CI$data[[i]])
}

print("FPR EVALUATION") # NOTE: WINNER HERE IS LOWER
FPR.comp.algorithm <- dat %>%
  dplyr::group_by(h2, algorithm, nQTL, group.size, CI) %>%
  yardstick::sens(truth = Simulated, estimate = Detected) %>%
  dplyr::mutate(.metric = "FPR") %>%
  tidyr::pivot_wider(names_from = algorithm, values_from = .estimate) %>%
  dplyr::select(-.metric, -.estimator) %>%
  dplyr::mutate(winner = if_else(condition = `LMM-EXACT-LOCO` >= `LMM-EXACT-INBRED`, 
                                 true = "INBRED",
                                 false = "LOCO")) %>%
  dplyr::group_by(group.size, CI) %>%
  tidyr::nest()

for(i in 1:length(FPR.comp.algorithm$data)){
  print(paste0("Group Size: ", as.character(FPR.comp.algorithm$group.size[[i]])))
  print(paste0("CI Extension: ", as.character(FPR.comp.algorithm$CI[[i]])))
  print(FPR.comp.algorithm$data[[i]])
}


FPR.comp.group.size <- dat %>%
  dplyr::group_by(h2, algorithm, nQTL, group.size, CI) %>%
  yardstick::sens(truth = Simulated, estimate = Detected) %>%
  dplyr::mutate(.metric = "FPR") %>%
  tidyr::pivot_wider(names_from = group.size, values_from = .estimate) %>%
  dplyr::select(-.metric, -.estimator) %>%
  dplyr::mutate(winner = if_else(condition = `500` >= `1000`,
                                 true = "1000",
                                 false = "500")) %>%
  dplyr::arrange(nQTL) %>%
  dplyr::group_by(algorithm, CI) %>%
  tidyr::nest()

for(i in 1:length(FPR.comp.group.size$data)){
  print(paste0("CI Extension: ", as.character(FPR.comp.group.size$CI[[i]])))
  print(paste0("Algorithm: ", as.character(FPR.comp.group.size$algorithm[[i]])))
  print(FPR.comp.group.size$data[[i]])
}

FPR.comp.CI <- dat %>%
  dplyr::group_by(h2, algorithm, nQTL, group.size, CI) %>%
  yardstick::sens(truth = Simulated, estimate = Detected) %>%
  dplyr::mutate(.metric = "FPR") %>%
  tidyr::pivot_wider(names_from = CI, values_from = .estimate) %>%
  dplyr::select(-.metric, -.estimator) %>%
  dplyr::mutate(winner = if_else(condition = `50` >= `150`,
                                 true = "150",
                                 false = "50")) %>%
  dplyr::arrange(nQTL) %>%
  dplyr::group_by(algorithm, group.size) %>%
  tidyr::nest()

for(i in 1:length(FPR.comp.CI$data)){
  print(paste0("Group Size: ", as.character(FPR.comp.CI$group.size[[i]])))
  print(paste0("Algorithm: ", as.character(FPR.comp.CI$algorithm[[i]])))
  print(FPR.comp.CI$data[[i]])
}

```

```{r unique hits, eval=FALSE, include=FALSE}
unique.hits <- dat.group1000.150 %>%
  dplyr::filter(Detected == TRUE) %>%
  dplyr::group_by(nQTL, h2, Rep, algorithm, group.size, CI) %>%
  tidyr::nest()

unique.hits.calc <- function(nQTL, Rep, h2, algorithm, group.size, CI, data){
  check <- strsplit(data$sim, split = "_")[[1]]
  # print(paste(nQTL == check[1], Rep == check[2], h2 == check[3]))
  data %>%
    dplyr::mutate(n_unique_hits = length(unique(as.factor(data$peak_id))), 
                  nQTL = nQTL, h2 = h2, group.size = group.size, CI = CI, Rep = Rep, algorithm = algorithm) %>%
    dplyr::select(nQTL, h2, Rep, algorithm, group.size, CI, n_unique_hits) %>%
    dplyr::filter(!duplicated(.))
}
unique.hits.dat <- purrr::pmap(.l = list(unique.hits$nQTL,
                      unique.hits$Rep,
                      unique.hits$h2,
                      unique.hits$algorithm,
                      unique.hits$group.size,
                      unique.hits$CI,
                      unique.hits$data),
            .f = unique.hits.calc) %>%
  Reduce(rbind,.)

unique.hits.dat %>%
  dplyr::group_by(nQTL, h2, algorithm, group.size, CI) %>%
  dplyr::summarise(round(mean(n_unique_hits), digits = 2), round(sd(n_unique_hits),digits = 2)) %>%
  `colnames<-`(c("nQTL","h2","algorithm","group.size","CI","mean","sd")) %>%
  tidyr::unite("unique_hits", mean:sd, sep = " Â± ") %>%
  tidyr::pivot_wider(names_from = nQTL, values_from = unique_hits) %>%
  as.data.frame()

unique.hits.plot <- unique.hits.dat %>%
  dplyr::group_by(nQTL, h2, algorithm, group.size, CI) %>%
  dplyr::summarise(round(mean(n_unique_hits), digits = 2), round(sd(n_unique_hits),digits = 2)) %>%
  `colnames<-`(c("nQTL","h2","algorithm","group.size","CI","mean","sd")) %>%
  ggplot(., mapping = aes(x = group.size, y = mean, colour = nQTL)) + 
  theme_bw() +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_errorbar(aes(ymin = mean - sd, ymax = mean + sd), 
                position=position_dodge(width=0.5)) +
  scale_colour_manual(values = nQTL.pal, name = "Number of Supporting QTL") +
  facet_grid(algorithm~h2+CI) +  
  theme(strip.text = element_text(size = 8),
        legend.position = "top", 
        panel.grid.minor = element_blank()) + 
  scale_y_continuous(breaks = seq(0,8,1)) + 
  labs(x = "Distance Between Grouped QTL",
       y = "Unique Detection Intervals")

unique.true.positives <- dat.group1000.150 %>%
  dplyr::filter(Simulated == TRUE, Detected == TRUE) %>%
  dplyr::group_by(nQTL, h2, Rep, algorithm, group.size, CI) %>%
  tidyr::nest()

unique.peak.calc <- function(nQTL, Rep, h2, algorithm, group.size, CI, data){
  check <- strsplit(data$sim, split = "_")[[1]]
  # print(paste(nQTL == check[1], Rep == check[2], h2 == check[3]))
  data %>%
    dplyr::mutate(peak_id = as.factor(peak_id)) %>%
    dplyr::group_by(peak_id) %>%
    dplyr::summarise(n()) %>%
    dplyr::select(`n()`) %>%
    as.vector() %>%
    dplyr::mutate(nQTL = nQTL, h2 = h2, Rep = Rep, algorithm = algorithm, group.size = group.size, CI = CI)
}
unique.true.positives.dat <- purrr::pmap(.l = list(unique.true.positives$nQTL,
                      unique.true.positives$Rep,
                      unique.true.positives$h2,
                      unique.true.positives$algorithm,
                      unique.true.positives$group.size,
                      unique.true.positives$CI,
                      unique.true.positives$data),
            .f = unique.peak.calc) %>%
  Reduce(rbind,.)

unique.true.positives.dat <- unique.true.positives.dat %>%
  dplyr::mutate(algorithm = as.factor(algorithm))
levels(unique.true.positives.dat$algorithm) <- c("INBRED","LOCO")

totals <- unique.true.positives.dat %>%
  dplyr::rename(QTL_per_peak = `n()`) %>%
  dplyr::group_by(h2, algorithm, group.size, CI) %>%
  dplyr::summarise(n())
unique.true.positives.dat <- unique.true.positives.dat %>%
  dplyr::mutate(algorithm = as.factor(algorithm))

QTL_per_peak_plot <- unique.true.positives.dat %>%
  dplyr::mutate(QTL_per_peak = as.numeric(`n()`)) %>%
  dplyr::group_by(nQTL, h2, algorithm, group.size, CI, QTL_per_peak) %>%
  dplyr::summarise(n()) %>%
  dplyr::rename(freq = `n()`) %>%
  dplyr::full_join(.,totals) %>%
  dplyr::mutate(pct = (freq/`n()`)*100) %>%
  dplyr::ungroup() %>%
  dplyr::mutate(QTL_per_peak = if_else(condition = (QTL_per_peak > 4),
                                       true = "5+",
                                       false = as.character(QTL_per_peak))) %>%
  ggplot(., mapping = aes(x = QTL_per_peak, y = pct, fill = nQTL)) +
  theme_bw() +
  geom_col(position = "stack") + 
  scale_fill_manual(values = nQTL.pal, name = "Number of Supporting QTL") +
  facet_grid(algorithm+group.size~h2+CI) +  
  theme(strip.text = element_text(size = 8),
        legend.position = "top", 
        panel.grid.minor = element_blank()) +
  labs(x = "Simulated QTL per Unique True Positive Detection Interval",
       y = "Simulated Regimes (%)")
```